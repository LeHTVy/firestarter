"""LangGraph workflow for pentest agent."""

from typing import Dict, Any, TypedDict, List, Optional, Callable
from langgraph.graph import StateGraph, END
from models.generic_ollama_agent import GenericOllamaAgent
from models.tool_calling_registry import get_tool_calling_registry
from agents.intent_classifier import IntentClassifier
from tools.executor import get_executor
from rag.results_storage import ToolResultsStorage
from rag.retriever import ConversationRetriever
from websearch.aggregator import SearchAggregator
from utils.input_normalizer import InputNormalizer
from memory.manager import get_memory_manager
from memory.manager import get_memory_manager
from agents.target_clarifier import TargetClarifier
from agents.mode_manager import ModeManager
from agents.nodes import (
    SubtaskCreator,
    AnalyzeNode,
    RecommendToolsNode,
    SynthesizeNode,
    TargetCheckNode
)


class GraphState(TypedDict):
    """State for LangGraph workflow."""
    user_prompt: str
    analysis: Optional[Dict[str, Any]]
    intent_classification: Optional[Dict[str, Any]]
    direct_answer: Optional[Dict[str, Any]]
    target_clarification: Optional[Dict[str, Any]]
    subtasks: List[Dict[str, Any]]
    selected_agent: Optional[str]
    tool_recommendations: Optional[Dict[str, Any]]
    user_approval: Optional[str]
    tool_results: List[Dict[str, Any]]
    search_results: Optional[Dict[str, Any]]
    knowledge_results: Optional[Dict[str, Any]]
    rag_results: Optional[List[Dict[str, Any]]]
    results_qa_answer: Optional[str]
    final_answer: Optional[str]
    session_id: Optional[str]
    conversation_id: Optional[str]
    conversation_history: List[Dict[str, Any]]


class PentestGraph:
    """LangGraph workflow for pentest agent."""
    
    def __init__(self, 
                 stream_callback: Optional[Callable[[str, str, Any], None]] = None,
                 analysis_model: str = "mistral:latest",
                 tool_calling_model: str = "json_tool_calling",
                 agent_model_config: Optional[Dict[str, str]] = None):
        """Initialize pentest graph.
        
        Args:
            stream_callback: Optional callback for streaming events
            analysis_model: Default model for task analysis
            tool_calling_model: Tool calling model to use
            agent_model_config: Optional dict mapping agent_name -> model_name
        """
        self.stream_callback = stream_callback
        self.agent_model_config = agent_model_config or {}
        self.tool_calling_model = tool_calling_model
        
        # Initialize core components
        self._init_models(analysis_model)
        self._init_managers()
        self._init_nodes()
        
        # Build graph
        self.graph = self._build_graph()
    
    def _init_models(self, default_model: str) -> None:
        """Initialize AI models.
        
        Model roles:
        - recon_agent: Fast model for initial task analysis and subtask creation
        - analysis_agent: Deep reasoning model for synthesizing tool results into conclusions
        """
        # Task analysis model - should be fast (e.g., mistral, qwen)
        task_analysis_model = self.agent_model_config.get("recon_agent", default_model)
        
        self.analysis_agent = GenericOllamaAgent(
            model_name=task_analysis_model,
            prompt_template="autogen_recon.jinja2"
        )
        self.analysis_model_name = task_analysis_model.replace(":", "_")
        
        
        # Synthesis model - used for reading tool results and providing conclusions
        synthesis_model = self.agent_model_config.get("analysis_agent", default_model)
        self.synthesis_agent = GenericOllamaAgent(
            model_name=synthesis_model,
            prompt_template="roles/analyzer.jinja2"
        )
        self.synthesis_model_name = synthesis_model.replace(":", "_")
        
        # AutoGen coordinator
        from agents.autogen_agents import AutoGenCoordinator
        self.autogen = AutoGenCoordinator(model_overrides=self.agent_model_config)
        
        # Tool calling registry
        self.tool_calling_registry = get_tool_calling_registry()
    
    def _init_managers(self) -> None:
        """Initialize managers and services."""
        self.executor = get_executor()
        self.results_storage = ToolResultsStorage()
        self.conversation_retriever = ConversationRetriever()
        self.search_aggregator = SearchAggregator()
        self.memory_manager = get_memory_manager()
        self.memory_manager = get_memory_manager()
        # self.context_manager = get_context_manager() # Removed
        # self.context_manager.set_memory_manager(self.memory_manager)
        
        self.intent_classifier = IntentClassifier(model_name=self.analysis_agent.model_name)
        
        # Results Q&A Agent (reads memory context)
        self.results_qa_agent = GenericOllamaAgent(
            model_name=self.analysis_agent.model_name,
            prompt_template="results_qa.jinja2"
        )
        
        # DirectAnswerAgent removed
        self.input_normalizer = InputNormalizer(ai_model=self.analysis_agent)
        
        # ScopeManager removed
        self.mode_manager = ModeManager()
        
        # Target clarifier
        self.target_clarifier = TargetClarifier(
            analysis_agent=self.analysis_agent,
            memory_manager=self.memory_manager,
            # context_manager=self.context_manager, # Removed
            stream_callback=self.stream_callback
        )
        
        # Tool executor node (includes policy validation)
        from agents.tool_executor_node import ToolExecutorNode
        self.tool_executor_node = ToolExecutorNode(
            # context_manager=self.context_manager, # Removed
            memory_manager=self.memory_manager,
            results_storage=self.results_storage,
            mode_manager=self.mode_manager,
            stream_callback=self.stream_callback,
            tool_calling_model=self.tool_calling_model
        )
        
        # Prompt optimizer removed
    
    def _init_nodes(self) -> None:
        """Initialize graph nodes."""
        # Subtask creator (also handles proactive planning)
        # self.subtask_creator = SubtaskCreator(
        #     context_manager=self.context_manager,
        #     stream_callback=self.stream_callback
        # )
        self.subtask_creator = SubtaskCreator(
            # context_manager=self.context_manager,
            # For now passing None or updating SubtaskCreator signature later
            # Assuming SubtaskCreator needs update too, skipping for now to fix AnalyzeNode first
            stream_callback=self.stream_callback
        )
        
        self.analyze_node = AnalyzeNode(
            analysis_agent=self.analysis_agent,
            analysis_model_name=self.analysis_model_name,
            synthesis_agent=self.synthesis_agent,
            memory_manager=self.memory_manager,
            # context_manager=self.context_manager, # Removed
            subtask_creator=self.subtask_creator,
            stream_callback=self.stream_callback
        )
        
        self.recommend_tools_node = RecommendToolsNode(
            memory_manager=self.memory_manager,
            mode_manager=self.mode_manager,
            stream_callback=self.stream_callback
        )
        
        self.synthesize_node = SynthesizeNode(
            synthesis_agent=self.synthesis_agent,
            tool_executor_node=self.tool_executor_node,
            memory_manager=self.memory_manager,
            stream_callback=self.stream_callback
        )
        
        self.target_check_node = TargetCheckNode(
            memory_manager=self.memory_manager,
            # context_manager=self.context_manager, # Removed, use memory manager
            input_normalizer=self.input_normalizer,
            stream_callback=self.stream_callback
        )
        
    
    def _build_graph(self) -> StateGraph:
        """Build LangGraph workflow."""
        workflow = StateGraph(GraphState)
        
        # Add nodes
        workflow.add_node("check_target", self._check_target_node)
        workflow.add_node("detect_confirmation", self._detect_confirmation_node)
        workflow.add_node("clarify_target", self._clarify_target_node)
        workflow.add_node("analyze", self._analyze_node)
        workflow.add_node("classify_intent", self._classify_intent_node)
        # workflow.add_node("direct_answer", self._direct_answer_node)
        workflow.add_node("select_agent", self._select_agent_node)
        workflow.add_node("recommend_tools", self._recommend_tools_node)
        workflow.add_node("execute_tools", self._execute_tools_node)
        workflow.add_node("web_search", self._web_search_node)
        workflow.add_node("knowledge_lookup", self._knowledge_lookup_node)
        workflow.add_node("rag_retrieval", self._rag_retrieval_node)
        workflow.add_node("results_qa", self._results_qa_node)
        workflow.add_node("synthesize", self._synthesize_node)
        
        # Define edges
        workflow.set_entry_point("check_target")
        workflow.add_conditional_edges("check_target", self._route_by_target_clarity,
            {"clear": "analyze", "ambiguous": "detect_confirmation"})
        workflow.add_conditional_edges("detect_confirmation", self._route_by_confirmation,
            {"confirmed": "analyze", "not_confirmed": "clarify_target"})
        workflow.add_conditional_edges("analyze", self._route_after_analyze,
            {"direct_tool": "select_agent", 
             "continue": "classify_intent",
             "results_qa": "results_qa",
             "synthesize": "synthesize"})
        workflow.add_edge("clarify_target", END)
        # DirectAnswerNode removed - route directly to select_agent
        workflow.add_edge("classify_intent", "select_agent")
        workflow.add_conditional_edges("select_agent", self._should_execute_tools,
            {"tools": "recommend_tools", "search": "web_search", 
             "knowledge": "knowledge_lookup", "rag": "rag_retrieval",
             "qa": "results_qa", "synthesize": "synthesize"})
        workflow.add_conditional_edges("recommend_tools", self._check_user_approval,
            {"approved": "execute_tools", "rejected": "synthesize", "pending": "execute_tools"})
        workflow.add_edge("execute_tools", "synthesize")
        workflow.add_edge("web_search", "synthesize")
        workflow.add_edge("knowledge_lookup", "synthesize")
        workflow.add_edge("rag_retrieval", "synthesize")
        workflow.add_edge("results_qa", "synthesize")
        workflow.add_edge("synthesize", END)
        
        return workflow.compile()
    
    # ==================== NODE METHODS ====================
    
    def _check_target_node(self, state: GraphState) -> GraphState:
        return self.target_check_node.check_target(state)
    
    def _detect_confirmation_node(self, state: GraphState) -> GraphState:
        return self.target_check_node.detect_confirmation(state)
    
    def _clarify_target_node(self, state: GraphState) -> GraphState:
        return self.target_clarifier.clarify_target(state)
    
    def _analyze_node(self, state: GraphState) -> GraphState:
        return self.analyze_node.execute(state)
    
    def _classify_intent_node(self, state: GraphState) -> GraphState:
        user_prompt = state["user_prompt"]
        conversation_history = state.get("conversation_history", [])
        
        context_prompt = user_prompt
        if conversation_history:
            recent = conversation_history[-3:]
            context_lines = [f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" for msg in recent]
            context_prompt = f"{user_prompt}\n\nPrevious conversation:\n" + "\n".join(context_lines)
        
        state["intent_classification"] = self.intent_classifier.classify(context_prompt)
        return state
    
    def _select_agent_node(self, state: GraphState) -> GraphState:
        if hasattr(self, 'autogen') and self.autogen:
            user_prompt = state.get("user_prompt", "")
            analysis = state.get("analysis") or {}
            task_type = analysis.get("task_type", "recon") if isinstance(analysis, dict) else "recon"
            
            # Extract tools from subtasks to help with routing
            required_tools = []
            subtasks = state.get("subtasks", [])
            if subtasks:
                for subtask in subtasks:
                    tools = subtask.get("required_tools", [])
                    if isinstance(tools, list):
                        required_tools.extend(tools)
            
            agent_name = self.autogen.route_task(user_prompt, task_type, required_tools)
            state["selected_agent"] = agent_name or "recon_agent"
        else:
            state["selected_agent"] = "recon_agent"
        return state
    
    def _recommend_tools_node(self, state: GraphState) -> GraphState:
        return self.recommend_tools_node.execute(state)
    
    def _execute_tools_node(self, state: GraphState) -> GraphState:
        return self.tool_executor_node.execute(state)
    
    def _web_search_node(self, state: GraphState) -> GraphState:
        analysis = state.get("analysis")
        if not analysis or not isinstance(analysis, dict):
            return state
        
        resources = analysis.get("resources", {})
        if not resources and "analysis" in analysis:
            resources = analysis["analysis"].get("resources", {})
        
        queries = resources.get("web_search_queries", [])
        if queries:
            state["search_results"] = self.search_aggregator.search_multiple_queries(queries)
        return state
    
    def _knowledge_lookup_node(self, state: GraphState) -> GraphState:
        state["knowledge_results"] = {}
        return state
    
    def _rag_retrieval_node(self, state: GraphState) -> GraphState:
        conversation_id = state.get("conversation_id") or state.get("session_id")
        state["rag_results"] = self.conversation_retriever.retrieve_context(
            query=state["user_prompt"],
            session_id=conversation_id,
            conversation_id=state.get("conversation_id")
        )
        return state
    
    def _results_qa_node(self, state: GraphState) -> GraphState:
        """Handle Results Q&A (Memory Query) with Query Router."""
        user_prompt = state["user_prompt"]
        conversation_id = state.get("conversation_id") or state.get("session_id")
        
        # 1. Routing (Heuristic based on keywords for efficiency)
        intent_type = "fact_query" # Default
        
        lower_prompt = user_prompt.lower()
        if any(w in lower_prompt for w in ["history", "previous", "earlier", "last time", "past", "snapshot", "context", "similar", "like this"]):
            intent_type = "semantic_query"
        elif any(w in lower_prompt for w in ["ips", "subdomains", "ports", "vulnerabilities", "count", "list", "show", "what", "find"]):
            intent_type = "fact_query"
        elif any(w in lower_prompt for w in ["session", "recall", "what did you do", "actions", "full log", "summary"]):
            intent_type = "session_recall"
            
        if self.stream_callback:
            self.stream_callback("model_response", "system", f"ğŸ¤” Query Router: Classified as '{intent_type}'")
            
        # 2. Retrieval
        retrieved_context = ""
        
        if intent_type == "fact_query":
            # Retrieve from Findings Store
            try:
                findings = self.memory_manager.conversation_store.get_findings(conversation_id)
                if findings:
                    retrieved_context = f"Structured Findings ({len(findings)} items):\n"
                    # Group by type
                    by_type = {}
                    for f in findings:
                        if f['type'] not in by_type: by_type[f['type']] = []
                        by_type[f['type']].append(f)
                    
                    for ftype, items in by_type.items():
                        if ftype == "open_ports":
                            retrieved_context += "- PORTS & SERVICES:\n"
                            # Increase limit for mass scans
                            for p_find in items[:200]: 
                                p = p_find['value']
                                if isinstance(p, dict):
                                    state = p.get('state', 'open').upper()
                                    retrieved_context += f"  * [{state}] {p.get('host')}:{p.get('port')} ({p.get('service')} {p.get('version')})\n"
                                else:
                                    retrieved_context += f"  * {p}\n"
                            if len(items) > 200:
                                retrieved_context += f"  * (+{len(items)-200} more ports)\n"
                        else:
                            # Higher limit for subdomains/IPs to ensure visibility in large infrastructure
                            limit = 500
                            values = [str(f['value']) for f in items]
                            retrieved_context += f"- {ftype.upper()}: {', '.join(values[:limit])}"
                            if len(values) > limit: 
                                retrieved_context += f" (+{len(values)-limit} more)"
                            retrieved_context += "\n"
                else:
                    retrieved_context = "No structured findings found for this session yet. This may mean the tools haven't finished or haven't found anything."
            except Exception as e:
                retrieved_context = f"Error retrieving findings: {e}"
                
        elif intent_type == "semantic_query":
             # Retrieve from Vector Store (Snapshots)
             try:
                 snapshots = self.memory_manager.context_manager.recall_similar_snapshots(user_prompt, k=3)
                 if snapshots:
                     retrieved_context = "Relevant Past Snapshots:\n"
                     for i, snap in enumerate(snapshots, 1):
                         retrieved_context += f"\n--- Snapshot {i} ---\n{snap.page_content}\n"
                 else:
                     retrieved_context = "No relevant past context found."
             except Exception as e:
                 retrieved_context = f"Error retrieving snapshots: {e}"
                 
        elif intent_type == "session_recall":
             # Retrieve full tool results history AND structured findings
             try:
                 # 1. Structured Findings (High Value)
                 findings = self.memory_manager.conversation_store.get_findings(conversation_id)
                 retrieved_context = "--- Structured Findings ---\n"
                 if findings:
                     for finding in findings:
                         retrieved_context += f"- [{finding['type']}] {finding['value']} (Confidence: {finding['confidence']})\n"
                 else:
                     retrieved_context += "No structured findings yet.\n"
                 
                 # 2. Tool Execution Logs (Raw Data)
                 results = self.memory_manager.conversation_store.get_tool_results(conversation_id)
                 retrieved_context += "\n--- Tool Execution History ---\n"
                 if results:
                     for res in results[-10:]: # Last 10
                         retrieved_context += f"- Tool: {res['tool_name']}\n  Command: {res['command']}\n  Output Summary: {res['stdout'][:200]}...\n"
                 else:
                     retrieved_context += "No tool execution history found."
                     
             except Exception as e:
                 retrieved_context = f"Error retrieving session history: {e}"

        # 3. Answer Synthesis
        model_callback = None
        if self.stream_callback:
            def callback(chunk: str):
                self.stream_callback("model_response", "results_qa_agent", chunk)
            model_callback = callback
            
        result = self.results_qa_agent.analyze(
            target=state.get("session_context", {}).get("target_domain"),
            previous_results=retrieved_context,
            task=user_prompt,
            stream_callback=model_callback
        )
        
        if result.get("success"):
            state["results_qa_answer"] = result.get("analysis")
        else:
            state["results_qa_answer"] = f"Failed to analyze results: {result.get('error')}"
            
        return state
    
    def _synthesize_node(self, state: GraphState) -> GraphState:
        return self.synthesize_node.execute(state)
    
    # ==================== ROUTING METHODS ====================
    
    def _route_by_target_clarity(self, state: GraphState) -> str:
        session_id = state.get("session_id")
        verified_target = self.memory_manager.get_verified_target(session_id)
        
        if verified_target:
            clarification = state.get("target_clarification", {})
            clarification["is_ambiguous"] = False
            clarification["verified_domain"] = verified_target
            state["target_clarification"] = clarification
            
            if self.memory_manager.session_memory:
                 self.memory_manager.session_memory.agent_context.domain = verified_target
                 state["session_context"] = self.memory_manager.session_memory.agent_context.to_dict()
            return "clear"
        
        clarification = state.get("target_clarification", {})
        return "ambiguous" if clarification.get("is_ambiguous", False) else "clear"
    
    def _route_by_confirmation(self, state: GraphState) -> str:
        clarification = state.get("target_clarification", {})
        return "confirmed" if clarification.get("verified_domain") else "not_confirmed"
    
    def _route_after_analyze(self, state: GraphState) -> str:
        if state.get("memory_context") or state.get("analysis", {}).get("intent_type") == "memory_query":
            return "results_qa"
            
        subtasks = state.get("subtasks", [])
        for subtask in subtasks:
            if subtask.get("type") == "tool_execution":
                return "direct_tool"
        return "continue"
    
    def _check_user_approval(self, state: GraphState) -> str:
        approval = state.get("user_approval")
        if approval is None:
            recommendations = state.get("tool_recommendations", {})
            if recommendations.get("needs_approval") and recommendations.get("tools"):
                state["_auto_approved"] = True
                return "approved"
            return "pending"
        return "approved" if approval.lower() in ["yes", "y", "approve", "ok", "okay"] else "rejected"
    
    def _should_execute_tools(self, state: GraphState) -> str:
        subtasks = state.get("subtasks", [])
        
        for subtask in subtasks:
            subtask_type = subtask.get("type", "")
            if subtask_type == "tool_execution":
                return "tools"
            elif subtask_type == "web_search":
                return "search"
            elif subtask_type == "knowledge_lookup":
                return "knowledge"
            elif subtask_type == "rag_retrieval":
                return "rag"
        
        user_prompt = state.get("user_prompt", "")
        if "result" in user_prompt.lower() or "káº¿t quáº£" in user_prompt.lower():
            return "qa"
        
        return "synthesize"
    
    # ==================== PUBLIC METHODS ====================
    
    def run(self, user_prompt: str, session_id: Optional[str] = None, 
            conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """Run workflow (non-streaming)."""
        initial_state = self._create_initial_state(user_prompt, session_id, conversation_id)
        final_state = self.graph.invoke(initial_state)
        return self._format_result(final_state, session_id)
    
    def run_streaming(self, user_prompt: str, session_id: Optional[str] = None,
                     conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """Run workflow with streaming."""
        initial_state = self._create_initial_state(user_prompt, session_id, conversation_id)
        self._load_conversation_context(initial_state)
        
        final_state = None
        current_state = initial_state.copy()
        
        for event in self.graph.stream(current_state):
            for node_name, node_state in event.items():
                if self.stream_callback:
                    self.stream_callback("state_update", node_name, node_state)
                
                if node_name == "recommend_tools":
                    recommendations = node_state.get("tool_recommendations")
                    if recommendations and recommendations.get("needs_approval") and node_state.get("user_approval") is None:
                        node_state["_needs_approval"] = True
                        final_state = node_state
                        break
                
                final_state = node_state
                current_state = node_state.copy()
        
        if final_state is None:
            final_state = current_state
        
        result = self._format_result(final_state, session_id)
        result["needs_approval"] = final_state.get("_needs_approval", False)
        result["approval_state"] = final_state if final_state.get("_needs_approval") else None
        return result
    
    def run_streaming_with_approval(self, user_prompt: str, session_id: Optional[str] = None,
                                    conversation_id: Optional[str] = None,
                                    ask_user_approval: Optional[Callable[[Dict[str, Any]], str]] = None) -> Dict[str, Any]:
        return self.run_streaming(user_prompt, session_id, conversation_id)
    
    def _create_initial_state(self, user_prompt: str, session_id: Optional[str],
                             conversation_id: Optional[str]) -> GraphState:
        return {
            "user_prompt": user_prompt,
            "analysis": None,
            "intent_classification": None,
            "direct_answer": None,
            "target_clarification": None,
            "subtasks": [],
            "selected_agent": None,
            "tool_recommendations": None,
            "user_approval": None,
            "tool_results": [],
            "search_results": None,
            "knowledge_results": None,
            "rag_results": None,
            "results_qa_answer": None,
            "final_answer": None,
            "session_id": session_id,
            "conversation_id": conversation_id or session_id,
            "conversation_history": []
        }
    
    def _load_conversation_context(self, state: GraphState) -> None:
        conv_id = state.get("conversation_id") or state.get("session_id")
        if not conv_id:
            return
        
        state["conversation_history"] = self.memory_manager.get_conversation_buffer(
            session_id=conv_id,
            conversation_id=state.get("conversation_id")
        )
        
        verified_target = self.memory_manager.get_verified_target(
            session_id=conv_id,
            conversation_id=state.get("conversation_id")
        )
        
        if self.memory_manager.session_memory:
             agent_context = self.memory_manager.session_memory.agent_context
             if verified_target:
                 agent_context.domain = verified_target
             state["session_context"] = agent_context.to_dict()
    
    def _format_result(self, state: Dict[str, Any], session_id: Optional[str]) -> Dict[str, Any]:
        return {
            "success": True,
            "answer": state.get("final_answer", ""),
            "tool_results": state.get("tool_results", []),
            "search_results": state.get("search_results"),
            "knowledge_results": state.get("knowledge_results"),
            "session_id": session_id
        }
